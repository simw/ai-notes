# Overview

## In general

In a general problem, we want to make a decision based on some input data. We focus here on making a single, isolated decision using a static input dataset. The input data could be numbers, text, audio, pictures, video, some other form of data, or a mixture of these.  Sometimes we want to make the decision based just on the input data, but often the data can be split into input variables and target variables, and the new decision comes with new values of the input variables. While it is sometimes possible to combine modeling and decision making, more often we will separate the problem into two stages: first, modeling a probability distribution of variables given the data (the inference stage), and then making a decision based on the probability distribution (the decision stage). While the second stage may be very important, we will focus most of our time here on the first, modeling the probability distribution.

## Simplifying the problem

We begin by expressing the probability distribution as a sum (or integral) over all possible models and model parameterizations, weighted by our priors for the models and parameters. In practice, our prior will usually be zero except for one specific model, and the result is a sum (or integral) over the parameters (or 'weights') of that model. However, this sum or integral is usually too difficult to evaluate analytically, and too expensive to calculate numerically. Further, we may not even have a firm view on our prior. Hence, we usually assume that the likelihood of the input data given weights is very strongly peaked and that we can focus only on the maximum likelihood solution. Finding the maximum of the likelihood will be much easier to evaluate than the full integral.

## Further analytical work

Even when pursuing a numerical solution of the maximum likelihood, further analytical work at this point can make the numerical work less expensive and more robust. For example, when predicting continuous target variables from continuous inputs (a regression problem), assuming that the input data is iid and the target variable is a deterministic function of the inputs plus a Gaussian noise term independent of the input variable(s) transforms the calculation into minimizing the least squares error - irrespective of how the deterministic function has been constructed. In the discrete target variable case, when the target can be expressed as 1-of-K, then maximizing the likelihood can be simplified to minimizing the cross-entropy error.

## Simple parameterization

At this point, we have to decide how to parameterize the probability distribution as a function of the input variables. Almost any functional form for the probability distribution can be optimized with stochastic gradient descent to find the maximum likelihood (albeit with various degrees of numerical efficiency or success). But for simpler problems, very strong assumptions can be useful - either because those assumptions are valid for the problem at hand, or to develop intuition or produce intuitive results. In a regression problem, assuming a linear relationship between input and target variables (or a linear relationship after a basis transformation) leads to linear regression, where calculating the parameters becomes a matrix evaluation from the data, no numerical optimization needed. In the discrete target variable case, assuming a sigmoid form for the probability does not lead to a closed-form solution, but to a form that can be efficiently evaluated using an iterative approach. A wide range of models can be useful, especially for low-dimensional data.

## Neural networks

For more complicated problems, we want to use a parameterization for the probability that is very general. There are many possible approximations to functions - eg Taylor expansion, Fourier series - but these often become unwieldy or inefficient when the input and / or target variables are very high dimensional. A parameterization that works in high (and low) dimensions is that of neural networks. These express the output as a sum over non-linear functions of the inputs, where the non-linear functions are weighted and shifted by the parameters of the model. While a single layer of these functions can in theory approximate any function to any desired precision (given enough parameters), multiple layers of functions (aka deep networks) are much more effective in many practical problems. These models can even be extended to infinite series of inputs by introducing recurrance, feeding back outputs from the model as inputs for the next evaluation.

The key is not on what can theoretically be represented, but what can be fit through optimization (usually a variant of stochastic gradient descent) in practice, and how well a model generalizes beyond the given data. While these deep networks allow very general relationships between input and target variables, it is still the case that feature engineering, encoding the strucutre of the problem in the structure of the network and normalization steps can lead to improvements in the convergence properties of the optimization, even turning infeasible problems into feasible ones.

## Latent variables

So far we have talked about parameterization of the probability distribution through a direct functional form. Another approach is to express the distribution either through a change of variables or as a sum or integral over other variables, known as a latent variable representation. Again, while a neural network can in theory fit any probability distribution, this latent variable representation might be easier to optimize, and generalize better from finite data, if it encodes some of the structure of the problem. Simpler versions of these models (eg linear latent variables) can be attacked analytically. However, for fully non-linear latent variables, stochastic gradient descent may run into difficulties. For example, the change of variables may not exist for all values of the parameters (eg if defined as an inverse of a function), or if every numerical evaluation of the distribution requires calculating an integral. Hence, other approaches are needed to fit the parameters to data.

## Decision making

We have left the decision making to the end. As mentioned, some approaches directly incorporate the decision making into the modeling (by finding a discriminant function), but the more flexible approach is to do the decision making after inference. Often this is simple - take an expected value, or sample from the distribution - but more involved approaches can involve utility or other derived functions.

In the above, we have focused on the case of a static dataset used to make a single decision. This approach may also be sufficient for multiple sequential decisions if those decisions can be treated as independent - although additional work may be needed to search in large state spaces for the best series of decisions. But if the decisions are not independent, then we will need to consider the link between the policy used to make the decisions and the probability distribution of outcomes - not just on how our current decision affects the future, but on how our future decisions affect our current one.
